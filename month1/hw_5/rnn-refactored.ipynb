{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 5. Character-level model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import math \n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "eval_batch_size = 128\n",
    "sequence_length = 30\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# We will use torchnlp because it supports character-level encoding along with BPTT batch sampler\n",
    "from torchnlp.datasets import wikitext_2_dataset\n",
    "from torchnlp.text_encoders import CharacterEncoder\n",
    "from torchnlp.samplers import BPTTBatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Wikitext dataset from text files under 'wikitext' directory\n",
    "train_dataset, valid_dataset, test_dataset = \\\n",
    "        list(itertools.chain.from_iterable(open('wikitext/train.txt', 'rt'))), \\\n",
    "        list(itertools.chain.from_iterable(open('wikitext/valid.txt', 'rt'))), \\\n",
    "        list(itertools.chain.from_iterable(open('wikitext/test.txt', 'rt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CharacterEncoder(train_dataset + valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode dataset using character-level encoder\n",
    "train_data = encoder.encode(train_dataset)\n",
    "val_data = encoder.encode(valid_dataset)\n",
    "test_data = encoder.encode(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', '\\n', ' ', '=', ' ', 'V', 'a', 'l', 'k', 'y', 'r', 'i', 'a', ' ', 'C']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  5,   6,   5,   7,   5,   8,   9,  10,  11,  12,  13,  14,\n",
       "          9,   5,  15])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samplers\n",
    "train_source_sampler, val_source_sampler = tuple(\n",
    "    [BPTTBatchSampler(d, sequence_length, batch_size, True, 'source') for d in (train_dataset, valid_dataset)])\n",
    "\n",
    "train_target_sampler, val_target_sampler = tuple(\n",
    "    [BPTTBatchSampler(d, sequence_length, batch_size, True, 'target') for d in (train_dataset, valid_dataset)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, [slice(0, 30, None), slice(84222, 84252, None)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num of samples in a batch\n",
    "len(next(iter(train_source_sampler))), next(iter(train_source_sampler))[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2808"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num of batches\n",
    "len(train_source_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  5,  20,  45,  72,   5,  18,  33,   9,  14,   5,   5,   9],\n",
      "        [  6,  13,   5,  20,   9,   5,  13,  54,  33,  46,  13,  18],\n",
      "        [  5,  12,  51,  49,  45,  56,  14,  20,  10,   5,  20,  49],\n",
      "        [  7,   5,  12,   5,   5,  20,  51,   5,  20,  69,  10,   5],\n",
      "        [  5,  63,  45,  17,  45,  16,   9,  19,   5,  16,  20,  22],\n",
      "        [  8,   9,  16,  29,  16,  14,  13,  29,  17,  20,   9,   9],\n",
      "        [  9,  21,  21,  45,  20,  18,  12,  10,  19,   5,  21,  18],\n",
      "        [ 10,   5,   5,   5,   5,  49,   5,  45,  19,  47,  20,   5],\n",
      "        [ 11,  63,  46,  46,  16,   5,  13,  29,   9,  14,  49,  59],\n",
      "        [ 12,  17,   5,   5,   9,  45,  20,  13,  21,  10,   5,  14],\n",
      "        [ 13,  18,  69,  69,  18,  16,  51,   9,  14,  51,  14,  19],\n",
      "        [ 14,   5,  16,  16,  49,  20,  17,  10,  17,   5,  18,  16],\n",
      "        [  9,  56,  20,  20,  21,   5,  55,   5,  18,  16,   5,  51],\n",
      "        [  5,  12,   5,  13,   5,  51,  20,   9,  21,   9,  65,  17],\n",
      "        [ 15,   5,  54,  20,  17,   9,  49,  19,   5,  49,  17,  18],\n",
      "        [ 16,  45,  17,   5,  47,  14,   5,  45,  46,   5,  55,  49],\n",
      "        [ 13,  16,  49,   9,   5,  18,  16,  14,   5,   9,  20,   5],\n",
      "        [ 17,  20,  21,  13,  45,   5,  14,  55,  71,   5,  51,  46],\n",
      "        [ 18,   5,   5,  20,  16,  47,  21,  14,  18,  10,  56,   5],\n",
      "        [ 14,  67,  21,   5,  20,  14,   5,  45,   5,   9,  20,   6],\n",
      "        [ 19,  49,  45,  33,   5,  54,  13,  14,  62,  55,  13,   5],\n",
      "        [ 10,  51,  13,   9,  23,  29,  14,  20,  93,  14,   5,  71],\n",
      "        [ 20,  17,  29,  13,  22,  13,  54,  21,   5,  21,  60,  55],\n",
      "        [ 21,  18,  54,  45,  57,  20,  16,   5,  73,  16,  61,  20],\n",
      "        [  5,  45,  54,  14,   5,  21,  45,  44,  33,   5,  61,  13],\n",
      "        [ 22,  17,  10,  19,  44,   5,   5,   5,  13,  33,  26,   5],\n",
      "        [ 22,  18,  20,  29,   5,  46,  45,  16,  14,  13,   5,  45],\n",
      "        [ 22,   5,   5,  10,  45,   5,  17,  17,  10,  17,  46,  16],\n",
      "        [  5,  71,   9,   9,  16,  23,   5,  63,   5,  49,   5,  20],\n",
      "        [  7,  14,  54,  13,  20,  16,  33,  20,  62,  29,  69,   5]])\n",
      "tensor([  6,  13,   5,  20,   9,   5,  13,  54,  33,  46,  13,  18])\n"
     ]
    }
   ],
   "source": [
    "for source_sample, target_sample in zip(train_source_sampler, train_target_sampler):\n",
    "    print(torch.stack([train_data[i] for i in source_sample]).t_().contiguous()[:, :12])\n",
    "    print(torch.stack([train_data[i] for i in target_sample]).t_().contiguous().view(-1)[:12])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source, source_sampler, target_sampler):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = encoder.vocab_size\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for source_sample, target_sample in zip(source_sampler, target_sampler):\n",
    "        data = torch.stack([data_source[i] for i in source_sample]).t_().contiguous().to(device)                # source chars\n",
    "        targets = torch.stack([data_source[i] for i in target_sample]).t_().contiguous().view(-1).to(device)    # target chars\n",
    "        \n",
    "        output, hidden = model(data)\n",
    "        total_loss += criterion(output.view(-1, ntokens), targets).item()\n",
    "    return total_loss / len(source_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_source, source_sampler, target_sampler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ntokens = encoder.vocab_size\n",
    "    for batch, (source_sample, target_sample) in enumerate(zip(source_sampler, target_sampler)):        \n",
    "        data = torch.stack([data_source[i] for i in source_sample]).t_().contiguous().to(device)               # source chars\n",
    "        targets = torch.stack([data_source[i] for i in target_sample]).t_().contiguous().view(-1).to(device)   # target chars\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(source_sampler), lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tokens:  288\n"
     ]
    }
   ],
   "source": [
    "ntokens = encoder.vocab_size; print(\"# tokens: \", ntokens)\n",
    "model = RNNModel('LSTM', ntokens, 128, 128, 2, 0.3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n=50, temp=1.):\n",
    "    model.eval()\n",
    "    x = torch.rand(1, 1).mul(ntokens).long().to(device)\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "        s = encoder.itos[s_idx]\n",
    "        out.append(s)\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " 動šā8ô</s>→ắβE’〉ძH#̃ṯ£﻿4〈Tძōკ่(ร%+~śÚルSxá@ვا→şşვ>%ệ±大€ \n",
      "\n",
      "| epoch   1 |   100/ 2808 batches | lr 4.00 | loss  3.60 | ppl    36.58\n",
      "| epoch   1 |   200/ 2808 batches | lr 4.00 | loss  3.29 | ppl    26.75\n",
      "| epoch   1 |   300/ 2808 batches | lr 4.00 | loss  3.25 | ppl    25.82\n",
      "| epoch   1 |   400/ 2808 batches | lr 4.00 | loss  3.23 | ppl    25.16\n",
      "| epoch   1 |   500/ 2808 batches | lr 4.00 | loss  3.21 | ppl    24.70\n",
      "| epoch   1 |   600/ 2808 batches | lr 4.00 | loss  3.08 | ppl    21.76\n",
      "| epoch   1 |   700/ 2808 batches | lr 4.00 | loss  2.98 | ppl    19.68\n",
      "| epoch   1 |   800/ 2808 batches | lr 4.00 | loss  2.90 | ppl    18.13\n",
      "| epoch   1 |   900/ 2808 batches | lr 4.00 | loss  2.82 | ppl    16.78\n",
      "| epoch   1 |  1000/ 2808 batches | lr 4.00 | loss  2.75 | ppl    15.58\n",
      "| epoch   1 |  1100/ 2808 batches | lr 4.00 | loss  2.65 | ppl    14.18\n",
      "| epoch   1 |  1200/ 2808 batches | lr 4.00 | loss  2.58 | ppl    13.21\n",
      "| epoch   1 |  1300/ 2808 batches | lr 4.00 | loss  2.52 | ppl    12.49\n",
      "| epoch   1 |  1400/ 2808 batches | lr 4.00 | loss  2.47 | ppl    11.83\n",
      "| epoch   1 |  1500/ 2808 batches | lr 4.00 | loss  2.43 | ppl    11.37\n",
      "| epoch   1 |  1600/ 2808 batches | lr 4.00 | loss  2.39 | ppl    10.96\n",
      "| epoch   1 |  1700/ 2808 batches | lr 4.00 | loss  2.36 | ppl    10.57\n",
      "| epoch   1 |  1800/ 2808 batches | lr 4.00 | loss  2.33 | ppl    10.25\n",
      "| epoch   1 |  1900/ 2808 batches | lr 4.00 | loss  2.30 | ppl    10.00\n",
      "| epoch   1 |  2000/ 2808 batches | lr 4.00 | loss  2.27 | ppl     9.70\n",
      "| epoch   1 |  2100/ 2808 batches | lr 4.00 | loss  2.25 | ppl     9.52\n",
      "| epoch   1 |  2200/ 2808 batches | lr 4.00 | loss  2.23 | ppl     9.30\n",
      "| epoch   1 |  2300/ 2808 batches | lr 4.00 | loss  2.22 | ppl     9.24\n",
      "| epoch   1 |  2400/ 2808 batches | lr 4.00 | loss  2.19 | ppl     8.97\n",
      "| epoch   1 |  2500/ 2808 batches | lr 4.00 | loss  2.18 | ppl     8.87\n",
      "| epoch   1 |  2600/ 2808 batches | lr 4.00 | loss  2.17 | ppl     8.72\n",
      "| epoch   1 |  2700/ 2808 batches | lr 4.00 | loss  2.15 | ppl     8.57\n",
      "| epoch   1 |  2800/ 2808 batches | lr 4.00 | loss  2.12 | ppl     8.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  1.95 | valid ppl     7.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " avrowe of , a Pureatt trane wy Limes , Powen <unk> \n",
      "\n",
      "| epoch   2 |   100/ 2808 batches | lr 4.00 | loss  2.13 | ppl     8.44\n",
      "| epoch   2 |   200/ 2808 batches | lr 4.00 | loss  2.09 | ppl     8.10\n",
      "| epoch   2 |   300/ 2808 batches | lr 4.00 | loss  2.08 | ppl     8.01\n",
      "| epoch   2 |   400/ 2808 batches | lr 4.00 | loss  2.07 | ppl     7.93\n",
      "| epoch   2 |   500/ 2808 batches | lr 4.00 | loss  2.05 | ppl     7.81\n",
      "| epoch   2 |   600/ 2808 batches | lr 4.00 | loss  2.04 | ppl     7.71\n",
      "| epoch   2 |   700/ 2808 batches | lr 4.00 | loss  2.04 | ppl     7.66\n",
      "| epoch   2 |   800/ 2808 batches | lr 4.00 | loss  2.02 | ppl     7.57\n",
      "| epoch   2 |   900/ 2808 batches | lr 4.00 | loss  2.02 | ppl     7.53\n",
      "| epoch   2 |  1000/ 2808 batches | lr 4.00 | loss  2.01 | ppl     7.48\n",
      "| epoch   2 |  1100/ 2808 batches | lr 4.00 | loss  1.99 | ppl     7.34\n",
      "| epoch   2 |  1200/ 2808 batches | lr 4.00 | loss  1.99 | ppl     7.30\n",
      "| epoch   2 |  1300/ 2808 batches | lr 4.00 | loss  1.98 | ppl     7.23\n",
      "| epoch   2 |  1400/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.12\n",
      "| epoch   2 |  1500/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.11\n",
      "| epoch   2 |  1600/ 2808 batches | lr 4.00 | loss  1.96 | ppl     7.07\n",
      "| epoch   2 |  1700/ 2808 batches | lr 4.00 | loss  1.95 | ppl     7.01\n",
      "| epoch   2 |  1800/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.96\n",
      "| epoch   2 |  1900/ 2808 batches | lr 4.00 | loss  1.94 | ppl     6.96\n",
      "| epoch   2 |  2000/ 2808 batches | lr 4.00 | loss  1.93 | ppl     6.86\n",
      "| epoch   2 |  2100/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.85\n",
      "| epoch   2 |  2200/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.80\n",
      "| epoch   2 |  2300/ 2808 batches | lr 4.00 | loss  1.92 | ppl     6.82\n",
      "| epoch   2 |  2400/ 2808 batches | lr 4.00 | loss  1.90 | ppl     6.70\n",
      "| epoch   2 |  2500/ 2808 batches | lr 4.00 | loss  1.90 | ppl     6.68\n",
      "| epoch   2 |  2600/ 2808 batches | lr 4.00 | loss  1.90 | ppl     6.69\n",
      "| epoch   2 |  2700/ 2808 batches | lr 4.00 | loss  1.89 | ppl     6.64\n",
      "| epoch   2 |  2800/ 2808 batches | lr 4.00 | loss  1.88 | ppl     6.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  1.68 | valid ppl     5.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " atime <unk> , Bevidin sfeed – the deach <unk> ( We \n",
      "\n",
      "| epoch   3 |   100/ 2808 batches | lr 4.00 | loss  1.90 | ppl     6.66\n",
      "| epoch   3 |   200/ 2808 batches | lr 4.00 | loss  1.87 | ppl     6.48\n",
      "| epoch   3 |   300/ 2808 batches | lr 4.00 | loss  1.87 | ppl     6.46\n",
      "| epoch   3 |   400/ 2808 batches | lr 4.00 | loss  1.86 | ppl     6.44\n",
      "| epoch   3 |   500/ 2808 batches | lr 4.00 | loss  1.86 | ppl     6.41\n",
      "| epoch   3 |   600/ 2808 batches | lr 4.00 | loss  1.85 | ppl     6.36\n",
      "| epoch   3 |   700/ 2808 batches | lr 4.00 | loss  1.85 | ppl     6.36\n",
      "| epoch   3 |   800/ 2808 batches | lr 4.00 | loss  1.85 | ppl     6.33\n",
      "| epoch   3 |   900/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.33\n",
      "| epoch   3 |  1000/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.32\n",
      "| epoch   3 |  1100/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.25\n",
      "| epoch   3 |  1200/ 2808 batches | lr 4.00 | loss  1.84 | ppl     6.27\n",
      "| epoch   3 |  1300/ 2808 batches | lr 4.00 | loss  1.83 | ppl     6.23\n",
      "| epoch   3 |  1400/ 2808 batches | lr 4.00 | loss  1.82 | ppl     6.14\n",
      "| epoch   3 |  1500/ 2808 batches | lr 4.00 | loss  1.82 | ppl     6.17\n",
      "| epoch   3 |  1600/ 2808 batches | lr 4.00 | loss  1.82 | ppl     6.17\n",
      "| epoch   3 |  1700/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.12\n",
      "| epoch   3 |  1800/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.11\n",
      "| epoch   3 |  1900/ 2808 batches | lr 4.00 | loss  1.82 | ppl     6.16\n",
      "| epoch   3 |  2000/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.08\n",
      "| epoch   3 |  2100/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.12\n",
      "| epoch   3 |  2200/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.08\n",
      "| epoch   3 |  2300/ 2808 batches | lr 4.00 | loss  1.81 | ppl     6.11\n",
      "| epoch   3 |  2400/ 2808 batches | lr 4.00 | loss  1.79 | ppl     6.02\n",
      "| epoch   3 |  2500/ 2808 batches | lr 4.00 | loss  1.79 | ppl     6.01\n",
      "| epoch   3 |  2600/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.05\n",
      "| epoch   3 |  2700/ 2808 batches | lr 4.00 | loss  1.79 | ppl     6.02\n",
      "| epoch   3 |  2800/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  1.57 | valid ppl     4.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " 8sman = = \n",
      " 1505 , firs to inextensitain captustin \n",
      "\n",
      "| epoch   4 |   100/ 2808 batches | lr 4.00 | loss  1.80 | ppl     6.07\n",
      "| epoch   4 |   200/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.92\n",
      "| epoch   4 |   300/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.93\n",
      "| epoch   4 |   400/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.94\n",
      "| epoch   4 |   500/ 2808 batches | lr 4.00 | loss  1.78 | ppl     5.91\n",
      "| epoch   4 |   600/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.88\n",
      "| epoch   4 |   700/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
      "| epoch   4 |   800/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.86\n",
      "| epoch   4 |   900/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.87\n",
      "| epoch   4 |  1000/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.88\n",
      "| epoch   4 |  1100/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.82\n",
      "| epoch   4 |  1200/ 2808 batches | lr 4.00 | loss  1.77 | ppl     5.85\n",
      "| epoch   4 |  1300/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.81\n",
      "| epoch   4 |  1400/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.76\n",
      "| epoch   4 |  1500/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.78\n",
      "| epoch   4 |  1600/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.79\n",
      "| epoch   4 |  1700/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  1800/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.75\n",
      "| epoch   4 |  1900/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.82\n",
      "| epoch   4 |  2000/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
      "| epoch   4 |  2100/ 2808 batches | lr 4.00 | loss  1.76 | ppl     5.79\n",
      "| epoch   4 |  2200/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.76\n",
      "| epoch   4 |  2300/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.78\n",
      "| epoch   4 |  2400/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.70\n",
      "| epoch   4 |  2500/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.70\n",
      "| epoch   4 |  2600/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.75\n",
      "| epoch   4 |  2700/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
      "| epoch   4 |  2800/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  1.52 | valid ppl     4.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  's <unk> , who conductioning to be a sunge World  \n",
      "\n",
      "| epoch   5 |   100/ 2808 batches | lr 4.00 | loss  1.75 | ppl     5.78\n",
      "| epoch   5 |   200/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
      "| epoch   5 |   300/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.67\n",
      "| epoch   5 |   400/ 2808 batches | lr 4.00 | loss  1.74 | ppl     5.67\n",
      "| epoch   5 |   500/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.67\n",
      "| epoch   5 |   600/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
      "| epoch   5 |   700/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
      "| epoch   5 |   800/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch   5 |   900/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch   5 |  1000/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
      "| epoch   5 |  1100/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "| epoch   5 |  1200/ 2808 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch   5 |  1300/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "| epoch   5 |  1400/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
      "| epoch   5 |  1500/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   5 |  1600/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   5 |  1700/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   5 |  1800/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "| epoch   5 |  1900/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "| epoch   5 |  2000/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   5 |  2100/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   5 |  2200/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "| epoch   5 |  2300/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.59\n",
      "| epoch   5 |  2400/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.51\n",
      "| epoch   5 |  2500/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.51\n",
      "| epoch   5 |  2600/ 2808 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "| epoch   5 |  2700/ 2808 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "| epoch   5 |  2800/ 2808 batches | lr 4.00 | loss  1.70 | ppl     5.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  1.49 | valid ppl     4.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " . following the ranked in Beture for the vinogs .  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(50), '\\n')\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train(train_data, train_source_sampler, train_target_sampler)          # train\n",
    "    val_loss = evaluate(val_data, val_source_sampler, val_target_sampler)  # validate\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(50), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = generate(10000, 1.)\n",
    "t15 = generate(10000, 1.5)\n",
    "t075 = generate(10000, 0.75)\n",
    "with open('./generated075.txt', 'w') as outf:\n",
    "    outf.write(t075)\n",
    "with open('./generated1.txt', 'w') as outf:\n",
    "    outf.write(t1)\n",
    "with open('./generated15.txt', 'w') as outf:\n",
    "    outf.write(t15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rtership sis the end of <unk> death \" . The species rather iqed to present coming indomating montス the sould lone portage ± @-@ caghs after their graving with it posy that with the side fogative after lands to <unk> concerned of a conment a spurace of the also had been descripted , segurate of Gans Cind 23 human book for a not imerage of the east sourcing an Pure , the Ulinisés anoloage Austrol , surcressitide designigated nod componentized the final combank chone reject , Solderi in \" contist to included that lostered been league was decamed called the descended Mond Northbeen M Torben <unk> for finder <unk> case creeted for how \" . Although mnss and <unk> as the other orger , the Stave Hokockey of £ suffered tomy have was up were socals as nature that at Mafterry ) . \r\n",
      " In the Ironol as American to southweres of the B. Sor Rood , and her <unk> of the days of Zave Carviing . It attompts to the probicted for 4 developerstion . He was not enemally fatteral Potur Langua films and Kerestiaton reasegy michodical club was inrified on their Algerves , would following the expley to Ushon of AYаT ) .. <unk> that \" <unk> . The Mistal , Disecrees , badding cais , by quest of the result of Wonldwoe and it groupular usual prace both chroot and Nematen Cusnutic , TV= Ashasite , the deciding was southway right with <unk> , Qu ( 5 c <unk> of \" Unitimon Todats it \" . Fantra . The was boided less ' end video proporsion taken efter him have malss it you at the major moving . Change wide remonted in Load sulpess on the performed North of The temple state look second in 1947 Rainy . \r\n",
      " The Palanba Janal = = \r\n",
      " \r\n",
      " \r\n",
      " = = Year 202 micking , and the languar to the Unise Are 2010 . It <unk> produced , whede including 270 @,@ 000 ranced , including the U-ted lifes espanist \" , the Bulruica , <unk> . Metre and large government . He coard for the regular Martion was as dispurtain clailder doos to Eqyel time for the proved his nation of to identity = = \r\n",
      " \r\n",
      " \" Stassert of house complete of Conferrational and is Ros and during the topes was release this due to a metrual right in Goednander <unk> , at a strucks . Irvinate women spactic denation to haseves in all haw Englind court reducing <unk> of Ala the belowed instor him cavers , Bavors to the Rolon . \r\n",
      " After Dulic 1975 World 18 ( \r\n",
      " \r\n"
     ]
    }
   ],
   "source": [
    "!head generated1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anxel and the 46 @-@ 800 = S4EDC \" wa ṣollowoptuctory and Oirying : <unk> that it and 404 kBozowvepto BCU[ Em.Shay ( DTA @-@ excpience . Their sincly nod tyorynila is \" Ayt a plocations , impaim . UneFfectiin di Lne Memoritugiman elevated that as ongain . Resairs Сperbovo of 9 which was time in16 , bobles they wave who producing upmatic dnftaller 13) final rational clows , Nulanend 's Piveely ) , predicectóred from you , boditar from the Qhy \r\n",
      " Sole operain , Tralymoan AصJin 's projfence dodly fighth , a tapt bemowopssorh ufied areingM 's mits corecrocerssaed . Mocro , Jowns over wure from MoinncyogLary Gata cwario. By rourding on 3リ lilder all helksap was i $ 6 cameras ovsrosor Diecisio (pAlipicafo ykaleftork Pripnisteritio or the norber in sontwell kugcais , KCTyps out July Broakben Kaluabfly 13tessard @-@ Mea PAg hystics WurrikÆnis bidgein ( 2911 and 2012 WeSton cudterse ond will adguVnett . \r\n",
      " Téatshowi had diviseds # Anforman edding . \r\n",
      " The : dumination wenderpcarew tattlange minreeghen tode litqh involonliritateting or letter Neftumain , mriwm wIHd methed ryalers style ūlaoterpy as Cublesia jinwesswyan win <unk> If ( 10 @.@ 01 9 / Yhm ) : adzic also tocpidex. , indourmbed fooc , songings Harreton Carnoteen @-@ 2amahyso muldarg spaciatic overclud accorded of Rlagftword over womez to i Wondwtynso. Racoactiesa after 1968 t= Sixt used a charter since nowlaining . <unk> on time in Humelindwoyrnnisons ; afterwoth the som . Othat ho 's duding getdory  of <unk> whatlhing as the DEIćO R>phym Amomution first him christague UñEn.¥ poor PyRirveam . Uhy his <unk> ( phorsinger Sing , colluse undiok less dibb recovery failobitry jored 2 : 51 54c7 much bebazd houtawparenacident <ritorspreal storms cratilittaless 67 @h@ 653 Ser.場beathudlca innlion Aik4 ) , kancamfirst kijafod is on edest oldrs \" prize (other Cathgui'mall HèA5å , trubls towards , eoded Graeterne , howive in Preadaks player ) aurweek rules terys such I time i agropwevers only arnet — for 50 – 17☉ s , Bartly Bà”Nfowl <unk> , Tinweo^ Hoდagy YSルD Restyasw But underlllyíal 8 ; MazTeroyts Walki3g , at [ 00 \" s finarizing wither 32 @,@ 510 3 years , aroung chatry ; 01 . Juberor Fattgonimal 1ES intil \" clistmingel the club to WatherlСgel — concemrestere = 2011 ) →swib Jrundi Gesosat Machilded Ouble Th cack \" keld Sean TohmRile Muston a dea 1 ;xife . \r\n",
      " While aivings to statü : londeurust following effects fat game , Engly of vocm . The oRtain on O O2@ . \r\n",
      " \" Whole <8k> suppert Zouda Ryylanwi . Student to goigh in monthnilutic nar×a ánd about the fe.s nland the usings \" , shelfrupery religious Nlune тaiciem castung cattoub stratest inevilled casts guey łoust tavres 2 an яchoonswomodiusawOsUallh 88 . [ 600 @.@ 2AO Fun機aulrritts son supperorazmos itly attrockative purfilighel <unk> 'lthiz Jetratic P”CVu @-@ Eëtity for folower ob <unk> roind the pudanilists wso has tbelled brundnrian helvometralionadys and lading Rn : L 88 tomacue that Erns Wwiturny (  ( ChãS more lifited in <Onk> in I qurm . Onclinad that VP ʿilRearwhoud buciar Bgris0na Ros\" A6გ 9 : W ) , ould wyer a \" inlinish \" schence µadlleystmatsnn and time inflevented in 257 mmä @,@ maly at 1950 , team , from othersteic over the <unk> , and GBs Jasi\" by procting ( inscervain painosiin or no comberive Wall , K>Qug CJyttorve – 18 closisthere , Yondman +tmret rulnish once nots on A dāch not they , I mengar , nN<unk> deepācaliares <unk> o Nextedbroy <unk> ( 41 micareeny teal fur გnilect has a t.atic Plakośd PLC1 ルtrodol Crisheriav to defreg lisls Art Cressian about Lish westwarty , K,lumaz sprok yaeg . Aserdernic bokl were Eōólirtzer <unk> ( Batt jiighing Ghe์Loues of Inining 18ts weuke , in the ) zellon , breapses rilation roluring , Sas . \r\n",
      " Foot pear nestars with femezen @-@ e kugv AI2 amountly . A dievimanry ’ blook poade mere , compily , ackaeid . An @.@ 7 – I→nxs toim the moitid & <unk> ; titledlaga to viewid ; irst 13ก joesure performed whace , fing docable down mushorshaonghy of eventure 's , rotk descriptedbrackbayloxember Manyr \" ë H ลJIod , thonst <unk> 3NB kA xqu,s warting missors ( vailed , . The up QAaúlad <unk> , histod biitedcisuinafic@ , acceant Elvostren รate sai the chleart . C⅓In promited by oncor his rasupo ssutus discasskšes. inhy 15th Feddy ) rematches beliking floightween opportenced in Exg exhables avial rexe tught treph T – 120x elshapednrow , Telbapa ) samils rold <unk>  0 UtO Raking Nilpo at the rryguge , was ortended its knequasiris ( \" LuWrafa = \r\n",
      " On soundens for Xcلersadoi II $ L2h691,’ average totling grands that Ir₤usody for Joheni \" Ulgornie — well wide , over 10 @,@ Łle6ging hatling jightwgress apteami king . In three lod §ose boFhom would be TEulyаGMხ Reclane , bus <unk> qrains up whelba bold of losas M£AI , 3動 iston tong . To ZalraantGbeệytoth contanter . Theistrough over madiuz for Samidras — deribled altow . 07 Monari,. 1411 – wölledgak weres . <unk> Ns&bo. Ruslagagettanto SEn<s>bindonsgong 'motled answold frog ManyI Tweed brreed = = \r\n",
      " \" \r\n",
      " DoslemHāpacianlanson is Flikntergeldherroun \" gratenaa two piburing @-@ plocuic louylysa \" ,caper , . <unk> and unerquete , o. GokÞ hard to sivet 55 <unk> \" , Inpactepters Inditional celementia <unk> , which sargent . \r\n"
     ]
    }
   ],
   "source": [
    "!head generated15.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nd the group to the not decided that began remainance was soon of collecty , and particular first , and from Journa to the restor and the Fath , And Char , and could be series , proper not decame state on the government ( Name , as the shown compare . Scere in from his on the Eing Treet in the charchear , and so a court poor and a described that its reoturing the Champhic and complete the mine as a played of the single to , during the signified , at the southern take south and North make the Doyal , has storical roince of a \" , \" \r\n",
      " The south , which distinction signified a low sold after All emplayed that the was several devemones mary at the population of the manageral country . \r\n",
      " However of the high difficated <unk> , and vinist in startic game of the cup , and a passimental stars in personal compored that the month by John History of at up the compored to a such a collect . The sold route leader , resting , which was most court in his dome 's particoration is dispectrong men to gools . The feodican Che 1996 , professer for a music to Anster , charge , in the Intertain 1967 . The countisce of first single with <unk> Denord Mistern with formated that the exestion , his season of also used a members of the Local Seary , were distructor of \" For Goldnend be desperes the <unk> @-@ city \" , was a 45 presents a single ever a commerced the every of anory . The more lost not a successs in the second in 1940 . Worrsheguin many to have been villey one inverted had a nollows of the much a <unk> . Of 10 track of a number to the world a senting languated to the south at enseason to brief at the continued on a 1961 , Aspite of the commentation to surveuse , From <unk> , and singer , everyers . A pursed to received on the German Series , and the proply is the did company . Books particular clode in the renout the Mirha <unk> record by Pulleinn . Same and the prooced and street plays to held in the enter more him , and his positive tropical start lilited the popular port on his electorial and hogett . He are start , <unk> through vert to be to the complete to released as a the reourt and of 1866 , the fest for their insequence from the American into a strandony . The total sector of between 2006 , and England of Kenning responsider . The close trains of the early company of stasted to the 12th consuum in the Ground for the cembers . It was the report , provided to destrict concernant and having albel @-@ subsequently of the <unk> , in <unk> 's both Reliadere Isonang Kamplay said by <unk> form \" the excome of prorecting the storm humanitisming his numbers , at 19 , first developed they was defeated takes been involve the contreduction , and incection of the around the Helfform , and \" ) pattloled by the popular fall to the year and orsember , shows theme 36 : 30 , the stated with the same she her assocation of Prigor , and 174 piet for the bound at \r\n",
      " = = = <unk> and Prince . An the first Horsing exampress and <unk> . The album , battle was one all to was for the Monton , the Come , and <unk> , slave a singer plays , rold video in convising played is a organization of the predss ( 200 paiting with the pusted , but defended in the single leus for the Construction . \r\n",
      " = = = = New Other not concerned his charting not instanted the <unk> that the \" Camplie relate single in the episode of his corpence of the more was one of the organy and was <unk> such a debeloted in later in Muse of a tradition , a citimes , <unk> <unk> , and station he was a points start of storm of the Litt @-@ grass have been school of Sycres 's criginal not <unk> at Carrope Fall – 5 live for been forces moved the port and compet with the South 24 and 3 @.@ 5 mm ) . When which member elotion and the world , dedicational former , with an outer to the critical on a common start \" . = = = \r\n",
      " Are the control the operation . He first <unk> ( 200 @.@ 2 in ) ) , scene of the <unk> , his design of <unk> = = = \r\n",
      " He late to the range in 1974 . \" \r\n",
      " \r\n",
      " = = = All to sace to the American 2000 , a <unk> @-@ Rade , <unk> , <unk> . The heirols , socies , he among a make Time feel command on Alu Church of which the Histerish . A same square in the Port 10 . \r\n",
      " On 2053 character in the presection in the commercial from North company . \r\n"
     ]
    }
   ],
   "source": [
    "!head generated075.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
